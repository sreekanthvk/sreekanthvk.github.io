<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Career Page">
    <meta name="author" content="Anjith George">

    <title>Anjith George - Homepage</title>

    <!-- Bootstrap Core CSS -->
    <link href="css/bootstrap.css" rel="stylesheet">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">

    <!-- Custom CSS -->
    <link href="css/1-col-portfolio.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-69511238-1', 'auto');
      ga('send', 'pageview');

    </script>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>
<body data-gr-c-s-loaded="true" data-feedly-mini="yes">

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href=""><img src="Figures/anjith.png" alt="Anjith George"></a>
      </div>
      <div class="author-name">Anjith George</div>
      <p>Computer Vision &amp; Machine Learning Researcher</p>
      <article class="article-page"><div class="page-content"><div class="wrap-content"><header class="header-page">
      <h3 class="page-sidebarmenu"><a href="index.html">About</a></h3>
      &nbsp; &nbsp; <h3 class="page-sidebarmenu"><a href="research.html">Research</a></h3>
      &nbsp; &nbsp; <h3 class="page-sidebarmenu"><a href="publications.html">Publications</a></h3>
      &nbsp; &nbsp; <h3 class="page-sidebarmenu"><a href="resume.html">CV</a></h3>
      &nbsp; &nbsp; <h3 class="page-sidebarmenu"><a href="resources.html">Resources</a></h3>
      &nbsp; &nbsp; <h3 class="page-sidebarmenu"><a href="demos.html">Demos</a></h3>

      </header></div></div></article>
    </div>
  </header> <!-- End Header -->  
  




    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
        
          <li class="github"><a href="http://github.com/anjith2006" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
          <li class="linkedin"><a href="https://ch.linkedin.com/in/anjith-george-ph-d-87402529" target="_blank"><i class="fa fa-linkedin"></i></a></li>
        
        
          <li><a href="https://www.youtube.com/channel/UCF-4bAPWrVmkLxQRTjDz0yQ" target="_blank"><i class="fa fa-youtube" aria-hidden="true"></i></a></li>
        
        
          <li class="email"><a href="mailto:anjith2006@gmail.com"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
      <p>  2024 © Anjith George  </p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->
<div class="content-box clearfix">





        <style>
          details, h2 { margin-bottom: 2px; }
          .doi-link { color: green; }
          .bold { font-weight: bold; }
          .abstract-box {
              padding: 10px;
              margin-top: 5px;
              background-color: #f9f9f9;
              border-left: 5px solid #2196F3; /* Blue */
              font-style: italic;
          }
          </style>
          </head>
          <body><h2>2024</h2><details>
                      <summary><span class='bold'>Anjith George</span>, Sébastien Marcel.
              <span class="bold">Heterogeneous Face Recognition Using Domain Invariant Units</span>.
              <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, 2024.
              <a href="https://doi.org/10.1109/ICASSP48485.2024.10447481" class="doi-link">doi:10.1109/ICASSP48485.2024.10447481</a> | <a href="https://www.semanticscholar.org/paper/0f54089f10453d2643ad1924b4bee7d5604103de" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Heterogeneous Face Recognition (HFR) aims to expand the applicability of Face Recognition (FR) systems to challenging scenarios, enabling the matching of face images across different domains, such as matching thermal images to visible spectra. However, the development of HFR systems is challenging because of the significant domain gap between modalities and the lack of availability of large-scale paired multi-channel data. In this work, we leverage a pretrained face recognition model as a teacher network to learn domain-invariant network layers called Domain-Invariant Units (DIU) to reduce the domain gap. The proposed DIU can be trained effectively even with a limited amount of paired training data, in a contrastive distillation framework. This proposed approach has the potential to enhance pretrained models, making them more adaptable to a wider range of variations in data. We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, Sébastien Marcel.
              <span class="bold">Modality Agnostic Heterogeneous Face Recognition with Switch Style Modulators</span>.
              <em>arXiv.org</em>, 2024.
              <a href="https://doi.org/10.48550/arXiv.2407.08640" class="doi-link">doi:10.48550/arXiv.2407.08640</a> | <a href="https://www.semanticscholar.org/paper/293ee4c3470d3687936fde5afb97eca9ec8b12ad" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Heterogeneous Face Recognition (HFR) systems aim to enhance the capability of face recognition in challenging cross-modal authentication scenarios. However, the significant domain gap between the source and target modalities poses a considerable challenge for cross-domain matching. Existing literature primarily focuses on developing HFR approaches for specific pairs of face modalities, necessitating the explicit training of models for each source-target combination. In this work, we introduce a novel framework designed to train a modality-agnostic HFR method capable of handling multiple modalities during inference, all without explicit knowledge of the target modality labels. We achieve this by implementing a computationally efficient automatic routing mechanism called Switch Style Modulation Blocks (SSMB) that trains various domain expert modulators which transform the feature maps adaptively reducing the domain gap. Our proposed SSMB can be trained end-to-end and seamlessly integrated into pre-trained face recognition models, transforming them into modality-agnostic HFR models. We have performed extensive evaluations on HFR benchmark datasets to demonstrate its effectiveness. The source code and protocols will be made publicly available.</div>
                    </details><details>
                      <summary>A. Unnervik, Hatef Otroshi Shahreza, <span class='bold'>Anjith George</span>, Sébastien Marcel.
              <span class="bold">Model Pairing Using Embedding Translation for Backdoor Attack Detection on Open-Set Classification Tasks</span>.
              <em>arXiv.org</em>, 2024.
              <a href="https://doi.org/10.48550/arXiv.2402.18718" class="doi-link">doi:10.48550/arXiv.2402.18718</a> | <a href="https://www.semanticscholar.org/paper/3a0bcbc09aa9ef49b35eca246853b47817d2f0ce" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Backdoor attacks allow an attacker to embed a specific vulnerability in a machine learning algorithm, activated when an attacker-chosen pattern is presented, causing a specific misprediction. The need to identify backdoors in biometric scenarios has led us to propose a novel technique with different trade-offs. In this paper we propose to use model pairs on open-set classification tasks for detecting backdoors. Using a simple linear operation to project embeddings from a probe model's embedding space to a reference model's embedding space, we can compare both embeddings and compute a similarity score. We show that this score, can be an indicator for the presence of a backdoor despite models being of different architectures, having been trained independently and on different datasets. Additionally, we show that backdoors can be detected even when both models are backdoored. The source code is made available for reproducibility purposes.</div>
                    </details><details>
                      <summary>Hatef Otroshi-Shahreza, Christophe Ecabert, <span class='bold'>Anjith George</span>, A. Unnervik, Sébastien Marcel, Nicolò Di Domenico, Guido Borghi, Davide Maltoni, Fadi Boutros, Julia Vogel, N. Damer, Ángela Sánchez-Pérez, Enrique Mas-Candela, Jorge Calvo-Zaragoza, Bernardo Biesseck, Pedro Vidal, Roger Granada, David Menotti, Ivan Deandres-Tame, Simone Maurizio La Cava, S. Concas, Pietro Melzi, Rubén Tolosana, R. Vera-Rodríguez, Gianpaolo Perelli, G. Orrú, G. L. Marcialis, Julian Fiérrez.
              <span class="bold">SDFR: Synthetic Data for Face Recognition Competition</span>.
              <em>IEEE International Conference on Automatic Face & Gesture Recognition</em>, 2024.
              <a href="https://doi.org/10.1109/FG59268.2024.10581946" class="doi-link">doi:10.1109/FG59268.2024.10581946</a> | <a href="https://www.semanticscholar.org/paper/877c7f64b53fd634ecf2914fe666ba1898be014e" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Large-scale face recognition datasets are collected by crawling the Internet and without individuals' consent, raising legal, ethical, and privacy concerns. With the recent advances in generative models, recently several works proposed generating synthetic face recognition datasets to mitigate concerns in web-crawled face recognition datasets. This paper presents the summary of the Synthetic Data for Face Recognition (SDFR) Competition held in conjunction with the 18th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2024) and established to investigate the use of synthetic data for training face recognition models. The SDFR competition was split into two tasks, allowing participants to train face recognition systems using new synthetic datasets and/or existing ones. In the first task, the face recognition backbone was fixed and the dataset size was limited, while the second task provided almost complete freedom on the model backbone, the dataset, and the training pipeline. The submitted models were trained on existing and also new synthetic datasets and used clever methods to improve training with synthetic data. The submissions were evaluated and ranked on a diverse set of seven benchmarking datasets. The paper gives an overview of the submitted face recognition models and reports achieved performance compared to baseline models trained on real and synthetic datasets. Furthermore, the evaluation of submissions is extended to bias assessment across different demography groups. Lastly, an outlook on the current state of the research in training face recognition models using synthetic data is presented, and existing problems as well as potential future directions are also discussed.</div>
                    </details><details>
                      <summary>Pietro Melzi, Rubén Tolosana, R. Vera-Rodríguez, Minchul Kim, C. Rathgeb, Xiaoming Liu, Ivan Deandres-Tame, A. Morales, Julian Fiérrez, J. Ortega-Garcia, Weisong Zhao, Xiangyu Zhu, Zheyu Yan, Xiao-Yu Zhang, Jinlin Wu, Zhen Lei, Suvidha Tripathi, Mahak Kothari, Md Haider Zama, Debayan Deb, Bernardo Biesseck, Pedro Vidal, Roger Granada, Guilherme Fickel, Gustavo Führ, David Menotti, A. Unnervik, <span class='bold'>Anjith George</span>, Christophe Ecabert, Hatef Otroshi-Shahreza, Parsa Rahimi, Sébastien Marcel, Ioannis Sarridis, C. Koutlis, Georgia Baltsou, Symeon Papadopoulos, Christos Diou, Nicolò Di Domenico, Guido Borghi, Lorenzo Pellegrini, Enrique Mas-Candela, Ángela Sánchez-Pérez, A. Atzori, Fadi Boutros, N. Damer, G. Fenu, M. Marras.
              <span class="bold">FRCSyn-onGoing: Benchmarking and comprehensive evaluation of real and synthetic data to improve face recognition systems</span>.
              <em>Information Fusion</em>, 2024.
              <a href="https://doi.org/10.1016/j.inffus.2024.102322" class="doi-link">doi:10.1016/j.inffus.2024.102322</a> | <a href="https://www.semanticscholar.org/paper/906ae08b5a8b7780dc7f641064abbc85e2e8e398" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> No abstract available.</div>
                    </details><details>
                      <summary>Ivan Deandres-Tame, Rubén Tolosana, Pietro Melzi, R. Vera-Rodríguez, Minchul Kim, C. Rathgeb, Xiaoming Liu, A. Morales, Julian Fiérrez, J. Ortega-Garcia, Zhizhou Zhong, Y. Huang, Yuxi Mi, Shouhong Ding, Shuigeng Zhou, Shuai He, Lingzhi Fu, Heng Cong, Rongyu Zhang, Zhihong Xiao, Evgeny Smirnov, Anton Pimenov, A.P. Grigorev, Denis Timoshenko, K. Asfaw, C. Low, Hao Liu, Chuyi Wang, Qing Zuo, Zhixiang He, Hatef Otroshi-Shahreza, <span class='bold'>Anjith George</span>, A. Unnervik, Parsa Rahimi, Sébastien Marcel, Pedro C. Neto, Marco Huber, J. Kolf, N. Damer, Fadi Boutros, Jaime S. Cardoso, Ana F. Sequeira, A. Atzori, G. Fenu, M. Marras, Vitomir Štruc, Jiang Yu, Zhangjie Li, Jichun Li, Weisong Zhao, Zhen Lei, Xiangyu Zhu, Xiao-Yu Zhang, Bernardo Biesseck, Pedro Vidal, Luiz Coelho, Roger Granada, David Menotti.
              <span class="bold">Second Edition FRCSyn Challenge at CVPR 2024: Face Recognition Challenge in the Era of Synthetic Data</span>.
              <em>arXiv.org</em>, 2024.
              <a href="https://doi.org/10.48550/arXiv.2404.10378" class="doi-link">doi:10.48550/arXiv.2404.10378</a> | <a href="https://www.semanticscholar.org/paper/9dff0667767bef59de9ced4b69968f443db88442" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> No abstract available.</div>
                    </details><details>
                      <summary>Pavel Korshunov, <span class='bold'>Anjith George</span>, Gökhan Özbulak, Sébastien Marcel.
              <span class="bold">Vulnerability of Face age Verification to Replay Attacks</span>.
              <em>IEEE International Conference on Acoustics, Speech, and Signal Processing</em>, 2024.
              <a href="https://doi.org/10.1109/ICASSP48485.2024.10447255" class="doi-link">doi:10.1109/ICASSP48485.2024.10447255</a> | <a href="https://www.semanticscholar.org/paper/b4cf5a2d30fd28abe3574df9befdaabb51288475" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Presentation attacks on biometric systems have long created significant security risks. The increase in the adoption of age verification systems, which ensure that only age-appropriate content is consumed online, raises the question of vulnerability of such systems to replay presentation attacks. In this paper, we analyze the vulnerability of face age verification to simple replay attacks and assess whether presentation attack detection (PAD) systems created for biometrics can be effective at detecting similar attacks on age verification. We used three types of attacks captured with iPhone 12, Galaxy S9, and Huawei Mate 30 phones from iPad Pro, which replayed the images from a commonly used UTKFace dataset of faces with true age labels. We evaluated four state of the art face age verification algorithms, including simple classification, distribution-based, regression via classification, and adaptive distribution approaches. We show that these algorithms are vulnerable to the attacks, since the accuracy of age verification on replayed images is only a couple of percentage points different compared to when the original images are used, which means an age verification system cannot distinguish attacks from bona fide images. Using two state of the art presentation attack detection systems, DeepPixBiS and CDCN, trained to detect similar attacks on biometrics, we demonstrate that they struggle to detect both: the types of attacks that are possible in age verification scenario and the type of bona fide images that are commonly used. These results highlight the need for the development of age verification specific attack detection systems for age verification to become practical.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, Sébastien Marcel.
              <span class="bold">From Modalities to Styles: Rethinking the Domain Gap in Heterogeneous Face Recognition</span>.
              <em>IEEE Transactions on Biometrics Behavior and Identity Science</em>, 2024.
              <a href="https://doi.org/10.1109/TBIOM.2024.3365350" class="doi-link">doi:10.1109/TBIOM.2024.3365350</a> | <a href="https://www.semanticscholar.org/paper/ccde56677f2dd82fb70f10f99ce2c1acd4a2ced2" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Heterogeneous Face Recognition (HFR) focuses on matching faces from different domains, for instance, thermal to visible images, making Face Recognition (FR) systems more versatile for challenging scenarios. However, the domain gap between these domains and the limited large-scale datasets in the target HFR modalities make it challenging to develop robust HFR models from scratch. In our work, we view different modalities as distinct styles and propose a method to modulate feature maps of the target modality to address the domain gap. We present a new Conditional Adaptive Instance Modulation (CAIM ) module that seamlessly fits into existing FR networks, turning them into HFR-ready systems. The CAIM block modulates intermediate feature maps, efficiently adapting to the style of the source modality and bridging the domain gap. Our method enables end-to-end training using a small set of paired samples. We extensively evaluate the proposed approach on various challenging HFR benchmarks, showing that it outperforms state-of-the-art methods. The source code and protocols for reproducing the findings will be made publicly available</div>
                    </details><h2>2023</h2><details>
                      <summary>Pietro Melzi, Rubén Tolosana, R. Vera-Rodríguez, Minchul Kim, C. Rathgeb, Xiaoming Liu, Ivan Deandres-Tame, A. Morales, Julian Fiérrez, J. Ortega-Garcia, Weisong Zhao, Xiangyu Zhu, Zheyu Yan, Xiao-Yu Zhang, Jinlin Wu, Zhen Lei, Suvidha Tripathi, Mahak Kothari, Md Haider Zama, Debayan Deb, Bernardo Biesseck, Pedro Vidal, R. Granada, Guilherme P. Fickel, Gustavo Fuhr, D. Menotti, A. Unnervik, <span class='bold'>Anjith George</span>, Christophe Ecabert, Hatef Otroshi Shahreza, Parsa Rahimi, Sébastien Marcel, Ioannis Sarridis, C. Koutlis, Georgia Baltsou, Symeon Papadopoulos, Christos Diou, Nicolò Di Domenico, Guido Borghi, Lorenzo Pellegrini, Enrique Mas-Candela, 'Angela S'anchez-P'erez, A. Atzori, Fadi Boutros, N. Damer, G. Fenu, M. Marras.
              <span class="bold">FRCSyn Challenge at WACV 2024: Face Recognition Challenge in the Era of Synthetic Data</span>.
              <em>2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)</em>, 2023.
              <a href="https://doi.org/10.1109/WACVW60836.2024.00100" class="doi-link">doi:10.1109/WACVW60836.2024.00100</a> | <a href="https://www.semanticscholar.org/paper/9b76e765170c4afb261ee7b073c28b63dc3a4eb1" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Despite the widespread adoption of face recognition technology around the world, and its remarkable performance on current benchmarks, there are still several challenges that must be covered in more detail. This paper offers an overview of the Face Recognition Challenge in the Era of Synthetic Data (FRCSyn) organized at WACV 2024. This is the first international challenge aiming to explore the use of synthetic data in face recognition to address existing limitations in the technology. Specifically, the FRCSyn Challenge targets concerns related to data privacy issues, demographic biases, generalization to unseen scenarios, and performance limitations in challenging scenarios, including significant age disparities between enrollment and testing, pose variations, and occlusions. The results achieved in the FRCSyn Challenge, together with the proposed benchmark, contribute significantly to the application of synthetic data to improve face recognition technology.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, Christophe Ecabert, Hatef Otroshi Shahreza, Ketan Kotwal, S. Marcel.
              <span class="bold">EdgeFace: Efficient Face Recognition Model for Edge Devices</span>.
              <em>IEEE Transactions on Biometrics Behavior and Identity Science</em>, 2023.
              <a href="https://doi.org/10.1109/TBIOM.2024.3352164" class="doi-link">doi:10.1109/TBIOM.2024.3352164</a> | <a href="https://www.semanticscholar.org/paper/0dd5a27cd22f5c4c4c336d51030ea157a4e2fa17" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> In this paper, we present EdgeFace - a lightweight and efficient face recognition network inspired by the hybrid architecture of EdgeNeXt. By effectively combining the strengths of both CNN and Transformer models, and a low rank linear layer, EdgeFace achieves excellent face recognition performance optimized for edge devices. The proposed EdgeFace network not only maintains low computational costs and compact storage, but also achieves high face recognition accuracy, making it suitable for deployment on edge devices. The proposed EdgeFace model achieved the top ranking among models with fewer than 2M parameters in the IJCB 2023 Efficient Face Recognition Competition. Extensive experiments on challenging benchmark face datasets demonstrate the effectiveness and efficiency of EdgeFace in comparison to state-of-the-art lightweight models and deep face recognition models. Our EdgeFace model with 1.77M parameters achieves state of the art results on LFW (99.73%), IJB-B (92.67%), and IJB-C (94.85%), outperforming other efficient models with larger computational complexities. The code to replicate the experiments will be made available publicly.</div>
                    </details><details>
                      <summary>ˇZ. Emerˇsiˇc, T. Ohki, M. Akasaka, T. Arakawa, S. Maeda, M. Okano, Y. Sato, <span class='bold'>Anjith George</span>, S. Marcel, I. I. Ganapathi, S. S. Ali, S. Javed, N. Werghi, S. G. Is¸ık, E. Sarıtas¸, H. K. Ekenel, V. Hudovernik, J. Kolf, F. Boutros, N. Damer, Grishma Sharma, A. Kamboj, A. Nigam, D. Jain, G. C´amara-Ch´avez, P. Peer, V. ˇStruc.
              <span class="bold">The Unconstrained Ear Recognition Challenge 2023: Maximizing Performance and Minimizing Bias*</span>.
              <em>2023 IEEE International Joint Conference on Biometrics (IJCB)</em>, 2023.
              <a href="https://doi.org/10.1109/IJCB57857.2023.10449062" class="doi-link">doi:10.1109/IJCB57857.2023.10449062</a> | <a href="https://www.semanticscholar.org/paper/1a9bb79070c73b2be6b39aa14ad64a1e0b13348c" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> The paper provides a summary of the 2023 Unconstrained Ear Recognition Challenge (UERC), a benchmarking effort focused on ear recognition from images acquired in uncontrolled environments. The objective of the challenge was to evaluate the effectiveness of current ear recognition techniques on a challenging ear dataset while analyzing the techniques from two distinct aspects, i.e., verification performance and bias with respect to specific demographic factors, i.e., gender and ethnicity. Seven research groups participated in the challenge and submitted a seven distinct recognition approaches that ranged from descriptor-based methods and deep-learning models to ensemble techniques that relied on multiple data representations to maximize performance and minimize bias. A comprehensive investigation into the performance of the submitted models is presented, as well as an in-depth analysis of bias and associated performance differentials due to differences in gender and ethnicity. The results of the challenge suggest that a wide variety of models (e.g., transformers, convolutional neural networks, ensemble models) is capable of achieving competitive recognition results, but also that all of the models still exhibit considerable performance differentials with respect to both gender and ethnicity. To promote further development of unbiased and effective ear recognition models, the starter kit of UERC 2023 together with the baseline model, and training and test data is made available from: http://ears.fri.uni-lj.si/</div>
                    </details><details>
                      <summary>J. Kolf, Fadi Boutros, Jurek Elliesen, Markus Theuerkauf, N. Damer, Mohamad Alansari, Oussama Abdul Hay, Sara Alansari, S. Javed, N. Werghi, Klemen Grm, Vitomir vStruc, F. Alonso-Fernandez, Kevin Hernandez Diaz, J. Bigun, <span class='bold'>Anjith George</span>, Christophe Ecabert, Hatef Otroshi Shahreza, Ketan Kotwal, S. Marcel, Iurii Medvedev, Bo-Hao Jin, D. Nunes, Ahmad Hassanpour, Pankaj Khatiwada, A. Toor, Bian Yang.
              <span class="bold">EFaR 2023: Efficient Face Recognition Competition</span>.
              <em>2023 IEEE International Joint Conference on Biometrics (IJCB)</em>, 2023.
              <a href="https://doi.org/10.1109/IJCB57857.2023.10448917" class="doi-link">doi:10.1109/IJCB57857.2023.10448917</a> | <a href="https://www.semanticscholar.org/paper/2581020be3d8ebbd3b7f95ed899439c13fc3fbea" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> This paper presents the summary of the Efficient Face Recognition Competition (EFaR) held at the 2023 International Joint Conference on Biometrics (IJCB 2023). The competition received 17 submissions from 6 different teams. To drive further development of efficient face recognition models, the submitted solutions are ranked based on a weighted score of the achieved verification accuracies on a diverse set of benchmarks, as well as the deployability given by the number of floating-point operations and model size. The evaluation of submissions is extended to bias, cross-quality, and large-scale recognition benchmarks. Overall, the paper gives an overview of the achieved performance values of the submitted solutions as well as a diverse set of baselines. The submitted solutions use small, efficient network architectures to reduce the computational cost, some solutions apply model quantization. An outlook on possible techniques that are underrepresented in current solutions is given as well.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">Bridging the Gap: Heterogeneous Face Recognition with Conditional Adaptive Instance Modulation</span>.
              <em>arXiv.org</em>, 2023.
              <a href="https://doi.org/10.48550/arXiv.2307.07032" class="doi-link">doi:10.48550/arXiv.2307.07032</a> | <a href="https://www.semanticscholar.org/paper/435ce28376d3895cabbdb3ff564d49177e088b75" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Heterogeneous Face Recognition (HFR) aims to match face images across different domains, such as thermal and visible spectra, expanding the applicability of Face Recognition (FR) systems to challenging scenarios. However, the domain gap and limited availability of large-scale datasets in the target domain make training robust and invariant HFR models from scratch difficult. In this work, we treat different modalities as distinct styles and propose a framework to adapt feature maps, bridging the domain gap. We introduce a novel Conditional Adaptive Instance Modulation (CAIM) module that can be integrated into pre-trained FR networks, transforming them into HFR networks. The CAIM block modulates intermediate feature maps, to adapt the style of the target modality effectively bridging the domain gap. Our proposed method allows for end-to-end training with a minimal number of paired samples. We extensively evaluate our approach on multiple challenging benchmarks, demonstrating superior performance compared to state-of-the-art methods. The source code and protocols for reproducing the findings will be made publicly available.</div>
                    </details><details>
                      <summary>Hatef Otroshi, <span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">SynthDistill: Face Recognition with Knowledge Distillation from Synthetic Data</span>.
              <em>2023 IEEE International Joint Conference on Biometrics (IJCB)</em>, 2023.
              <a href="https://doi.org/10.1109/IJCB57857.2023.10448642" class="doi-link">doi:10.1109/IJCB57857.2023.10448642</a> | <a href="https://www.semanticscholar.org/paper/4f3c83cd9a101a35bf5fb3bd4c86cbb67d8fae0c" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> State-of-the-art face recognition networks are often computationally expensive and cannot be used for mobile applications. Training lightweight face recognition models also requires large identity-labeled datasets. Meanwhile, there are privacy and ethical concerns with collecting and using large face recognition datasets. While generating synthetic datasets for training face recognition models is an alternative option, it is challenging to generate synthetic data with sufficient intra-class variations. In addition, there is still a considerable gap between the performance of models trained on real and synthetic data. In this paper, we propose a new framework (named SynthDistill) to train lightweight face recognition models by distilling the knowledge of a pretrained teacher face recognition model using synthetic data. We use a pretrained face generator network to generate synthetic face images and use the synthesized images to learn a lightweight student network. We use synthetic face images without identity labels, mitigating the problems in the intra-class variation generation of synthetic datasets. Instead, we propose a novel dynamic sampling strategy from the intermediate latent space of the face generator network to include new variations of the challenging images while further exploring new face images in the training batch. The results on five different face recognition datasets demonstrate the superiority of our lightweight model compared to models trained on previous synthetic datasets, achieving a verification accuracy of 99.52% on the LFW dataset with a lightweight network. The results also show that our proposed framework significantly reduces the gap between training with real and synthetic data. The source code for replicating the experiments is publicly released.</div>
                    </details><h2>2022</h2><details>
                      <summary>M. Ibsen, C. Rathgeb, Fabian Brechtel, Ruben Klepp, K. Pöppelmann, <span class='bold'>Anjith George</span>, S. Marcel, C. Busch.
              <span class="bold">Attacking Face Recognition With T-Shirts: Database, Vulnerability Assessment, and Detection</span>.
              <em>IEEE Access</em>, 2022.
              <a href="https://doi.org/10.1109/ACCESS.2023.3282780" class="doi-link">doi:10.1109/ACCESS.2023.3282780</a> | <a href="https://www.semanticscholar.org/paper/04b87436ed48481e5f5a23c84aa4dabc0a948a50" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Face recognition systems are widely deployed for biometric authentication. Despite this, it is well-known that, without any safeguards, face recognition systems are highly vulnerable to presentation attacks. In response to this security issue, several promising methods for detecting presentation attacks have been proposed which show high performance on existing benchmarks. However, an ongoing challenge is the generalization of presentation attack detection methods to unseen and new attack types. To this end, we propose a new T-shirt Face Presentation Attack (TFPA) database of 1,608 T-shirt attacks using 100 unique presentation attack instruments. In an extensive evaluation, we show that this type of attack can compromise the security of face recognition systems and that some state-of-the-art attack detection mechanisms trained on popular benchmarks fail to robustly generalize to the new attacks. Further, we propose three new methods for detecting T-shirt attack images, one which relies on the statistical differences between depth maps of bona fide images and T-shirt attacks, an anomaly detection approach trained on features only extracted from bona fide RGB images, and a fusion approach which achieves competitive detection performance.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">ROBUST FACE PRESENTATION ATTACK DETECTION WITH MULTI-CHANNEL NEURAL NETWORKS</span>.
              <em></em>, 2022.
              DOI not available | <a href="https://www.semanticscholar.org/paper/0d248a355dc4bd68b6952f708c351a32e22aca0a" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Vulnerability against presentation attacks remains a challenging issue limiting the reliable use of face recognition systems. Though several methods have been proposed in the literature for the detection of presentation attacks, the majority of these methods fail in generalizing to unseen attacks and environments. Since the quality of attack instruments keeps getting better, the difference between bonaﬁde and attack samples is diminishing making it harder to distinguish them using the visible spectrum alone. In this context, multi-channel presentation attack detection methods have been proposed as a solution to secure face recognition systems. Even with multiple channels, special care needs to be taken to ensure that the model generalizes well in challenging scenarios. In this chapter, we present three different strategies to use multi-channel information for presentation attack detection. Speciﬁcally, we present different architecture choices for fusion, along with ad-hoc loss functions as opposed to standard classiﬁcation objective. We conduct an extensive set of experiments in the HQ-WMCA dataset, which contains a wide variety of attacks and sensing channels together with challenging unseen attack evaluation protocols. We make the protocol, source codes, and data publicly available to enable further extensions of the work.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, A. Mohammadi, S. Marcel.
              <span class="bold">Prepended Domain Transformer: Heterogeneous Face Recognition Without Bells and Whistles</span>.
              <em>IEEE Transactions on Information Forensics and Security</em>, 2022.
              <a href="https://doi.org/10.1109/TIFS.2022.3217738" class="doi-link">doi:10.1109/TIFS.2022.3217738</a> | <a href="https://www.semanticscholar.org/paper/5b19954d7885665289a1f6aa51efd320f930b968" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Heterogeneous Face Recognition (HFR) refers to matching face images captured in different domains, such as thermal to visible images (VIS), sketches to visible images, near-infrared to visible, and so on. This is particularly useful in matching visible spectrum images to images captured from other modalities. Though highly useful, HFR is challenging because of the domain gap between the source and target domain. Often, large-scale paired heterogeneous face image datasets are absent, preventing training models specifically for the heterogeneous task. In this work, we propose a surprisingly simple, yet, very effective method for matching face images across different sensing modalities. The core idea of the proposed approach is to add a novel neural network block called Prepended Domain Transformer (PDT) in front of a pre-trained face recognition (FR) model to address the domain gap. Retraining this new block with few paired samples in a contrastive learning setup was enough to achieve state-of-the-art performance in many HFR benchmarks. The PDT blocks can be retrained for several source-target combinations using the proposed general framework. The proposed approach is architecture agnostic, meaning they can be added to any pre-trained FR models. Further, the approach is modular and the new block can be trained with a minimal set of paired samples, making it much easier for practical deployment. The source code and protocols will be made available publicly.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, David Geissbuhler, S. Marcel.
              <span class="bold">A Comprehensive Evaluation on Multi-channel Biometric Face Presentation Attack Detection</span>.
              <em>arXiv.org</em>, 2022.
              DOI not available | <a href="https://www.semanticscholar.org/paper/f4275d33cf5dd769cad91280adc9f491b949fe7b" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> The vulnerability against presentation attacks is a crucial problem undermining the wide-deployment of face recognition systems. Though presentation attack detection (PAD) systems try to address this problem, the lack of generalization and robustness continues to be a major concern. Several works have shown that using multi-channel PAD systems could alleviate this vulnerability and result in more robust systems. However, there is a wide selection of channels available for a PAD system such as RGB, Near Infrared, Shortwave Infrared, Depth, and Thermal sensors. Having a lot of sensors increases the cost of the system, and therefore an understanding of the performance of different sensors against a wide variety of attacks is necessary while selecting the modalities. In this work, we perform a comprehensive study to understand the effectiveness of various imaging modalities for PAD. The studies are performed on a multi-channel PAD dataset, collected with 14 different sensing modalities considering a wide range of 2D, 3D, and partial attacks. We used the multi-channel convolutional network-based architecture, which uses pixel-wise binary supervision. The model has been evaluated with different combinations of channels, and different image qualities on a variety of challenging known and unknown attack protocols. The results reveal interesting trends and can act as pointers for sensor selection for safety-critical presentation attack detection systems. The source codes and protocols to reproduce the results are made available publicly making it possible to extend this work to other architectures.</div>
                    </details><h2>2021</h2><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">Multi-channel Face Presentation Attack Detection Using Deep Learning</span>.
              <em>Advances in Computer Vision and Pattern Recognition</em>, 2021.
              <a href="https://doi.org/10.1007/978-3-030-74697-1_13" class="doi-link">doi:10.1007/978-3-030-74697-1_13</a> | <a href="https://www.semanticscholar.org/paper/3fef6918044af2dae915958465d304cbaaa7f23d" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> No abstract available.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">Cross Modal Focal Loss for RGBD Face Anti-Spoofing</span>.
              <em>Computer Vision and Pattern Recognition</em>, 2021.
              <a href="https://doi.org/10.1109/CVPR46437.2021.00779" class="doi-link">doi:10.1109/CVPR46437.2021.00779</a> | <a href="https://www.semanticscholar.org/paper/55ea1c014e914f79a072a3c5587faad50cd9a92b" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Automatic methods for detecting presentation attacks are essential to ensure the reliable use of facial recognition technology. Most of the methods available in the literature for presentation attack detection (PAD) fails in generalizing to unseen attacks. In recent years, multi-channel methods have been proposed to improve the robustness of PAD systems. Often, only a limited amount of data is available for additional channels, which limits the effectiveness of these methods. In this work, we present a new framework for PAD that uses RGB and depth channels together with a novel loss function. The new architecture uses complementary information from the two modalities while reducing the impact of overfitting. Essentially, a cross-modal focal loss function is proposed to modulate the loss contribution of each channel as a function of the confidence of individual channels. Extensive evaluations in two publicly available datasets demonstrate the effectiveness of the proposed approach.</div>
                    </details><details>
                      <summary>Sandip Purnapatra, Nic Smalt, Keivan Bahmani, Priyanka Das, David Yambay, A. Mohammadi, <span class='bold'>Anjith George</span>, T. Bourlai, S. Marcel, S. Schuckers, Meiling Fang, N. Damer, Fadi Boutros, Arjan Kuijper, Alperen Kantarci, Basar Demir, Zafer Yildiz, Zabi Ghafoory, Hasan Dertli, H. K. Ekenel, Son Vu, V. Christophides, Liang Dashuang, Zhang Guanghao, Hao Zhanlong, Liu Junfu, Jin Yufeng, Samo Liu, Samuel Huang, Salieri Kuei, Jag Mohan Singh, Raghavendra Ramachandra.
              <span class="bold">Face Liveness Detection Competition (LivDet-Face) - 2021</span>.
              <em>2021 IEEE International Joint Conference on Biometrics (IJCB)</em>, 2021.
              <a href="https://doi.org/10.1109/IJCB52358.2021.9484359" class="doi-link">doi:10.1109/IJCB52358.2021.9484359</a> | <a href="https://www.semanticscholar.org/paper/5e7251cb1341ffef0486ab534007b3cae0c7bc30" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Liveness Detection (LivDet)-Face is an international competition series open to academia and industry. The competition’s objective is to assess and report state-of-the-art in liveness / Presentation Attack Detection (PAD) for face recognition. Impersonation and presentation of false samples to the sensors can be classified as presentation attacks and the ability for the sensors to detect such attempts is known as PAD. LivDet-Face 2021 * will be the first edition of the face liveness competition. This competition serves as an important benchmark in face presentation attack detection, offering (a) an independent assessment of the current state of the art in face PAD, and (b) a common evaluation protocol, availability of Presentation Attack Instruments (PAI) and live face image dataset through the Biometric Evaluation and Testing (BEAT) platform. The competition can be easily followed by researchers after it is closed, in a platform in which participants can compare their solutions against the LivDet-Face winners.</div>
                    </details><h2>2020</h2><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">Can Your Face Detector Do Anti-spoofing? Face Presentation Attack Detection with a Multi-Channel Face Detector</span>.
              <em>arXiv.org</em>, 2020.
              DOI not available | <a href="https://www.semanticscholar.org/paper/07dd801b0f68081a88d1bac2093a36f35ead67ba" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> In a typical face recognition pipeline, the task of the face detector is to localize the face region. However, the face detector localizes regions that look like a face, irrespective of the liveliness of the face, which makes the entire system susceptible to presentation attacks. In this work, we try to reformulate the task of the face detector to detect real faces, thus eliminating the threat of presentation attacks. While this task could be challenging with visible spectrum images alone, we leverage the multi-channel information available from off the shelf devices (such as color, depth, and infrared channels) to design a multi-channel face detector. The proposed system can be used as a live-face detector obviating the need for a separate presentation attack detection module, making the system reliable in practice without any additional computational overhead. The main idea is to leverage a single-stage object detection framework, with a joint representation obtained from different channels for the PAD task. We have evaluated our approach in the multi-channel WMCA dataset containing a wide variety of attacks to show the effectiveness of the proposed framework.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">On the Effectiveness of Vision Transformers for Zero-shot Face Anti-Spoofing</span>.
              <em>2021 IEEE International Joint Conference on Biometrics (IJCB)</em>, 2020.
              <a href="https://doi.org/10.1109/IJCB52358.2021.9484333" class="doi-link">doi:10.1109/IJCB52358.2021.9484333</a> | <a href="https://www.semanticscholar.org/paper/15adfb10072ab06d1621c90f9ce844bd1d9b23cd" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> The vulnerability of face recognition systems to presentation attacks has limited their application in security-critical scenarios. Automatic methods of detecting such malicious attempts are essential for the safe use of facial recognition technology. Although various methods have been suggested for detecting such attacks, most of them over-fit the training set and fail in generalizing to unseen attacks and environments. In this work, we use transfer learning from the vision transformer model for the zero-shot anti-spoofing task. The effectiveness of the proposed approach is demonstrated through experiments in publicly available datasets. The proposed approach outperforms the state-of-the-art methods in the zero-shot protocols in the HQ-WMCA and SiW-M datasets by a large margin. Besides, the model achieves a significant boost in cross-database performance as well.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">Learning One Class Representations for Face Presentation Attack Detection Using Multi-Channel Convolutional Neural Networks</span>.
              <em>IEEE Transactions on Information Forensics and Security</em>, 2020.
              <a href="https://doi.org/10.1109/TIFS.2020.3013214" class="doi-link">doi:10.1109/TIFS.2020.3013214</a> | <a href="https://www.semanticscholar.org/paper/49bcbbcc957ea11634a8ace93d4a07ee813447af" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Face recognition has evolved as a widely used biometric modality. However, its vulnerability against presentation attacks poses a significant security threat. Though presentation attack detection (PAD) methods try to address this issue, they often fail in generalizing to unseen attacks. In this work, we propose a new framework for PAD using a one-class classifier, where the representation used is learned with a Multi-Channel Convolutional Neural Network (MCCNN). A novel loss function is introduced, which forces the network to learn a compact embedding for bonafide class while being far from the representation of attacks. A one-class Gaussian Mixture Model is used on top of these embeddings for the PAD task. The proposed framework introduces a novel approach to learn a robust PAD system from bonafide and available (known) attack classes. This is particularly important as collecting bonafide data and simpler attacks are much easier than collecting a wide variety of expensive attacks. The proposed system is evaluated on the publicly available WMCA multi-channel face PAD database, which contains a wide variety of 2D and 3D attacks. Further, we have performed experiments with MLFP and SiW-M datasets using RGB channels only. Superior performance in unseen attack protocols shows the effectiveness of the proposed approach. Software, data, and protocols to reproduce the results are made available publicly.</div>
                    </details><details>
                      <summary>G. Heusch, <span class='bold'>Anjith George</span>, David Geissbuhler, Z. Mostaani, S. Marcel.
              <span class="bold">Deep Models and Shortwave Infrared Information to Detect Face Presentation Attacks</span>.
              <em>IEEE Transactions on Biometrics Behavior and Identity Science</em>, 2020.
              <a href="https://doi.org/10.1109/TBIOM.2020.3010312" class="doi-link">doi:10.1109/TBIOM.2020.3010312</a> | <a href="https://www.semanticscholar.org/paper/8dbc9ddf5a863a5fb4ae335b80db97ec276290a5" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> This paper addresses the problem of face presentation attack detection using different image modalities. In particular, the usage of short wave infrared (SWIR) imaging is considered. Face presentation attack detection is performed using recent models based on Convolutional Neural Networks using only carefully selected SWIR image differences as input. Conducted experiments show superior performance over similar models acting on either color images or on a combination of different modalities (visible, NIR, thermal and depth), as well as on a SVM-based classifier acting on SWIR image differences. Experiments have been carried on a new public and freely available database, containing a wide variety of attacks. Video sequences have been recorded thanks to several sensors resulting in 14 different streams in the visible, NIR, SWIR and thermal spectra, as well as depth data. The best proposed approach is able to almost perfectly detect all impersonation attacks while ensuring low bonafide classification errors. On the other hand, obtained results show that obfuscation attacks are more difficult to detect. We hope that the proposed database will foster research on this challenging problem. Finally, all the code and instructions to reproduce presented experiments is made available to the research community.</div>
                    </details><details>
                      <summary>G. Heusch, <span class='bold'>Anjith George</span>, David Geissbühler, Z. Mostaani, S. Marcel.
              <span class="bold">High-Quality Wide Multi-Channel Attack (HQ-WMCA)</span>.
              <em></em>, 2020.
              <a href="https://doi.org/10.34777/0T0B-EZ97" class="doi-link">doi:10.34777/0T0B-EZ97</a> | <a href="https://www.semanticscholar.org/paper/ea6d2fab13967a02e4e70ea35bffa94542d9971f" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> No abstract available.</div>
                    </details><details>
                      <summary>Z. Mostaani, <span class='bold'>Anjith George</span>, G. Heusch, David Geissbuhler, S. Marcel.
              <span class="bold">The High-Quality Wide Multi-Channel Attack (HQ-WMCA) database</span>.
              <em>arXiv.org</em>, 2020.
              DOI not available | <a href="https://www.semanticscholar.org/paper/fcdf862bdf3c4cf3829b084998f14ed2c51d93cf" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> The High-Quality Wide Multi-Channel Attack database (HQ-WMCA) database extends the previous Wide Multi-Channel Attack database(WMCA), with more channels including color, depth, thermal, infrared (spectra), and short-wave infrared (spectra), and also a wide variety of attacks.</div>
                    </details><h2>2019</h2><details>
                      <summary><span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">Deep Pixel-wise Binary Supervision for Face Presentation Attack Detection</span>.
              <em>International Conference on Biometrics</em>, 2019.
              <a href="https://doi.org/10.1109/ICB45273.2019.8987370" class="doi-link">doi:10.1109/ICB45273.2019.8987370</a> | <a href="https://www.semanticscholar.org/paper/205cd9f45d788e9a03c335f782e36e20a5675ba9" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Face recognition has evolved as a prominent biometric authentication modality. However, vulnerability to presentation attacks curtails its reliable deployment. Automatic detection of presentation attacks is essential for secure use of face recognition technology in unattended scenarios. In this work, we introduce a Convolutional Neural Network (CNN) based framework for presentation attack detection, with deep pixel-wise supervision. The framework uses only frame level information making it suitable for deployment in smart devices with minimal computational and time overhead. We demonstrate the effectiveness of the proposed approach in public datasets for both intra as well as cross-dataset experiments. The proposed approach achieves an HTER of 0% in Replay Mobile dataset and an ACER of 0.42% in Protocol-1 of OULU dataset outperforming state of the art methods.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, Z. Mostaani, David Geissenbuhler, Olegs Nikisins, André Anjos, S. Marcel.
              <span class="bold">Biometric Face Presentation Attack Detection With Multi-Channel Convolutional Neural Network</span>.
              <em>IEEE Transactions on Information Forensics and Security</em>, 2019.
              <a href="https://doi.org/10.1109/TIFS.2019.2916652" class="doi-link">doi:10.1109/TIFS.2019.2916652</a> | <a href="https://www.semanticscholar.org/paper/50ec871d80e9894df66a0636fbbbe4430780dfc4" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Face recognition is a mainstream biometric authentication method. However, the vulnerability to presentation attacks (a.k.a. spoofing) limits its usability in unsupervised applications. Even though there are many methods available for tackling presentation attacks (PA), most of them fail to detect sophisticated attacks such as silicone masks. As the quality of presentation attack instruments improves over time, achieving reliable PA detection with visual spectra alone remains very challenging. We argue that analysis in multiple channels might help to address this issue. In this context, we propose a multi-channel Convolutional Neural Network-based approach for presentation attack detection (PAD). We also introduce the new Wide Multi-Channel presentation Attack (WMCA) database for face PAD which contains a wide variety of 2D and 3D presentation attacks for both impersonation and obfuscation attacks. Data from different channels such as color, depth, near-infrared, and thermal are available to advance the research in face PAD. The proposed method was compared with feature-based approaches and found to outperform the baselines achieving an ACER of 0.3% on the introduced dataset. The database and the software to reproduce the results are made available publicly.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>.
              <span class="bold">Image based Eye Gaze Tracking and its Applications</span>.
              <em>arXiv.org</em>, 2019.
              DOI not available | <a href="https://www.semanticscholar.org/paper/57e3e58fc4307559021da37ba0cf02bf860a5933" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Eye movements play a vital role in perceiving the world. Eye gaze can give a direct indication of the users point of attention, which can be useful in improving human-computer interaction. Gaze estimation in a non-intrusive manner can make human-computer interaction more natural. Eye tracking can be used for several applications such as fatigue detection, biometric authentication, disease diagnosis, activity recognition, alertness level estimation, gaze-contingent display, human-computer interaction, etc. Even though eye-tracking technology has been around for many decades, it has not found much use in consumer applications. The main reasons are the high cost of eye tracking hardware and lack of consumer level applications. In this work, we attempt to address these two issues. In the first part of this work, image-based algorithms are developed for gaze tracking which includes a new two-stage iris center localization algorithm. We have developed a new algorithm which works in challenging conditions such as motion blur, glint, and varying illumination levels. A person independent gaze direction classification framework using a convolutional neural network is also developed which eliminates the requirement of user-specific calibration. 
          In the second part of this work, we have developed two applications which can benefit from eye tracking data. A new framework for biometric identification based on eye movement parameters is developed. A framework for activity recognition, using gaze data from a head-mounted eye tracker is also developed. The information from gaze data, ego-motion, and visual features are integrated to classify the activities.</div>
                    </details><details>
                      <summary>Olegs Nikisins, <span class='bold'>Anjith George</span>, S. Marcel.
              <span class="bold">Domain Adaptation in Multi-Channel Autoencoder based Features for Robust Face Anti-Spoofing</span>.
              <em>International Conference on Biometrics</em>, 2019.
              <a href="https://doi.org/10.1109/ICB45273.2019.8987247" class="doi-link">doi:10.1109/ICB45273.2019.8987247</a> | <a href="https://www.semanticscholar.org/paper/612e2788d58fba4f6c0566a894934584f91d3812" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> While the performance of face recognition systems has improved significantly in the last decade, they are proved to be highly vulnerable to presentation attacks (spoofing). Most of the research in the field of face presentation attack detection (PAD), was focused on boosting the performance of the systems within a single database. Face PAD datasets are usually captured with RGB cameras, and have very limited number of both bona-fide samples and presentation attack instruments. Training face PAD systems on such data leads to poor performance, even in the closed-set scenario, especially when sophisticated attacks are involved. We explore two paths to boost the performance of the face PAD system against challenging attacks. First, by using multichannel (RGB, Depth and NIR) data, which is still easily accessible in a number of mass production devices. Second, we develop a novel Autoencoders + MLP based face PAD algorithm. Moreover, instead of collecting more data for training of the proposed deep architecture, the domain adaptation technique is proposed, transferring the knowledge of facial appearance from RGB to multi-channel domain. We also demonstrate, that learning the features of individual facial regions, is more discriminative than the features learned from an entire face. The proposed system is tested on a very recent publicly available multi-channel PAD database with a wide variety of presentation attacks.</div>
                    </details><h2>2018</h2><details>
                      <summary><span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">Recognition of Activities from Eye Gaze and Egocentric Video</span>.
              <em>arXiv.org</em>, 2018.
              DOI not available | <a href="https://www.semanticscholar.org/paper/acce6cd8b8a3f5ee010df757a82514060f9ff98f" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> This paper presents a framework for recognition of human activity from egocentric video and eye tracking data obtained from a head-mounted eye tracker. Three channels of information such as eye movement, ego-motion, and visual features are combined for the classification of activities. Image features were extracted using a pre-trained convolutional neural network. Eye and ego-motion are quantized, and the windowed histograms are used as the features. The combination of features obtains better accuracy for activity classification as compared to individual features.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">ESCaF: Pupil Centre Localization Algorithm with Candidate Filtering</span>.
              <em>arXiv.org</em>, 2018.
              DOI not available | <a href="https://www.semanticscholar.org/paper/e9fbecd700c27ebc1631cbd91f16cae4056038aa" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Algorithms for accurate localization of pupil centre is essential for gaze tracking in real world conditions. Most of the algorithms fail in real world conditions like illumination variations, contact lenses, glasses, eye makeup, motion blur, noise, etc. We propose a new algorithm which improves the detection rate in real world conditions. The proposed algorithm uses both edges as well as intensity information along with a candidate filtering approach to identify the best pupil candidate. A simple tracking scheme has also been added which improves the processing speed. The algorithm has been evaluated in Labelled Pupil in the Wild (LPW) dataset, largest in its class which contains real world conditions. The proposed algorithm outperformed the state of the art algorithms while achieving real-time performance.</div>
                    </details><h2>2017</h2><details>
                      <summary>Anwesha Sengupta, A. Dasgupta, Aritra Chaudhuri, <span class='bold'>Anjith George</span>, A. Routray, Rajlakshmi Guha.
              <span class="bold">A Multimodal System for Assessing Alertness Levels Due to Cognitive Loading</span>.
              <em>IEEE transactions on neural systems and rehabilitation engineering</em>, 2017.
              <a href="https://doi.org/10.1109/TNSRE.2017.2672080" class="doi-link">doi:10.1109/TNSRE.2017.2672080</a> | <a href="https://www.semanticscholar.org/paper/382f9960e86a94b991f7023b3d9570fed5e03fbc" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> This paper proposes a scheme for assessing the alertness levels of an individual using simultaneous acquisition of multimodal physiological signals and fusing the information into a single metric for quantification of alertness. The system takes electroencephalogram, high-speed image sequence, and speech data as inputs. Certain parameters are computed from each of these measures as indicators of alertness and a metric is proposed using a fusion of the parameters for indicating alertness level of an individual at an instant. The scheme has been validated experimentally using standard neuropsychological tests, such as the Visual Response Test (VRT), Auditory Response Test (ART), a Letter Counting (LC) task, and the Stroop Test. The tests are used both as cognitive tasks to induce mental fatigue as well as tools to gauge the present degree of alertness of the subject. Correlation between the measures has been studied and the experimental variables have been statistically analyzed using measures such as multivariate linear regression and analysis of variance. Correspondence of trends obtained from biomarkers and neuropsychological measures validate the usability of the proposed metric.</div>
                    </details><details>
                      <summary>Žiga Emeršič, Dejan Štepec, V. Štruc, P. Peer, <span class='bold'>Anjith George</span>, Adil Ahmad, E. Omar, T. Boult, Reza Safdari, Yuxiang Zhou, S. Zafeiriou, Dogucan Yaman, Fevziye Irem Eyiokur, H. K. Ekenel.
              <span class="bold">The unconstrained ear recognition challenge</span>.
              <em>2017 IEEE International Joint Conference on Biometrics (IJCB)</em>, 2017.
              <a href="https://doi.org/10.1109/BTAS.2017.8272761" class="doi-link">doi:10.1109/BTAS.2017.8272761</a> | <a href="https://www.semanticscholar.org/paper/8181bf57f235ac8ee02c27cc28781174a19c342b" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> In this paper we present the results of the Unconstrained Ear Recognition Challenge (UERC), a group benchmarking effort centered around the problem of person recognition from ear images captured in uncontrolled conditions. The goal of the challenge was to assess the performance of existing ear recognition techniques on a challenging large-scale dataset and identify open problems that need to be addressed in the future. Five groups from three continents participated in the challenge and contributed six ear recognition techniques for the evaluation, while multiple baselines were made available for the challenge by the UERC organizers. A comprehensive analysis was conducted with all participating approaches addressing essential research questions pertaining to the sensitivity of the technology to head rotation, flipping, gallery size, large-scale recognition and others. The top performer of the UERC was found to ensure robust performance on a smaller part of the dataset (with 180 subjects) regardless of image characteristics, but still exhibited a significant performance drop when the entire dataset comprising 3,704 subjects was used for testing.</div>
                    </details><h2>2016</h2><details>
                      <summary><span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">Real-time eye gaze direction classification using convolutional neural network</span>.
              <em>International Conference on Signal Processing and Communications</em>, 2016.
              <a href="https://doi.org/10.1109/SPCOM.2016.7746701" class="doi-link">doi:10.1109/SPCOM.2016.7746701</a> | <a href="https://www.semanticscholar.org/paper/4a0f98d7dbc31497106d4f652968c708f7da6692" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Estimation eye gaze direction is useful in various human-computer interaction tasks. Knowledge of gaze direction can give valuable information regarding users point of attention. Certain patterns of eye movements known as eye accessing cues are reported to be related to the cognitive processes in the human brain. We propose a real-time framework for the classification of eye gaze direction and estimation of eye accessing cues. In the first stage, the algorithm detects faces using a modified version of the Viola-Jones algorithm. A rough eye region is obtained using geometric relations and facial landmarks. The eye region obtained is used in the subsequent stage to classify the eye gaze direction. A convolutional neural network is employed in this work for the classification of eye gaze direction. The proposed algorithm was tested on Eye Chimera database and found to outperform state of the art methods. The computational complexity of the algorithm is very less in the testing phase. The algorithm achieved an average frame rate of 24 fps in the desktop environment.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">Fast and accurate algorithm for eye localisation for gaze tracking in low-resolution images</span>.
              <em>IET Computer Vision</em>, 2016.
              <a href="https://doi.org/10.1049/iet-cvi.2015.0316" class="doi-link">doi:10.1049/iet-cvi.2015.0316</a> | <a href="https://www.semanticscholar.org/paper/91ee88754cc7a193d51656a3b53e16389bf4aadb" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Iris centre (IC) localisation in low-resolution visible images is a challenging problem in computer vision community due to noise, shadows, occlusions, pose variations, eye blinks etc. This study proposes an efficient method for determining IC in low-resolution images in the visible spectrum. Even low-cost consumer-grade webcams can be used for gaze tracking without any additional hardware. A two-stage algorithm is proposed for IC localisation. The proposed method uses geometrical characteristics of the eye. In the first stage, a fast convolution-based approach is used for obtaining the coarse location of IC). The IC location is further refined in the second stage using boundary tracing and ellipse fitting. The algorithm has been evaluated in public databases such as BioID, Gi4E and is found to outperform the state-of-the-art methods.</div>
                    </details><details>
                      <summary>A. Morales, Julian Fierrez, M. Gomez-Barrero, J. Ortega-Garcia, Roberto Daza, John V. Monaco, J. Filho, J. Canuto, <span class='bold'>Anjith George</span>.
              <span class="bold">KBOC: Keystroke biometrics OnGoing competition</span>.
              <em>2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems (BTAS)</em>, 2016.
              <a href="https://doi.org/10.1109/BTAS.2016.7791180" class="doi-link">doi:10.1109/BTAS.2016.7791180</a> | <a href="https://www.semanticscholar.org/paper/9cd2e53785c59666eeb2d82f6bbeaa930dbabc10" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> This paper presents the first Keystroke Biometrics Ongoing evaluation platform and a Competition (KBOC) organized to promote reproducible research and establish a baseline in person authentication using keystroke biometrics. The ongoing evaluation tool has been developed using the BEAT platform and includes keystroke sequences (fixed-text) from 300 users acquired in 4 different sessions. In addition, the results of a parallel offline competition based on the same data and evaluation protocol are presented. The results reported have achieved EERs as low as 5.32%, which represent a challenging baseline for keystroke recognition technologies to be evaluated on the new publicly available KBOC benchmark.</div>
                    </details><details>
                      <summary>Anwesha Sengupta, <span class='bold'>Anjith George</span>, A. Dasgupta, Aritra Chaudhuri, Bibek Kabi, A. Routray.
              <span class="bold">Alertness Monitoring System for Vehicle Drivers using Physiological Signals</span>.
              <em></em>, 2016.
              <a href="https://doi.org/10.4018/978-1-5225-0084-1.CH013" class="doi-link">doi:10.4018/978-1-5225-0084-1.CH013</a> | <a href="https://www.semanticscholar.org/paper/b13b367519a76c3c7020873bb13950b6ceae68ef" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> The present chapter deals with the development of a robust real-time embedded system which can detect the level of drowsiness in automotive and locomotive drivers based on ocular images and speech signals of the driver. The system has been cross-validated using Electroencephalogram (EEG) as well as Psychomotor response tests. A ratio based on eyelid closure rates called PERcentage of eyelid CLOSure (PERCLOS) using Principal Component Analysis (PCA) and Support Vector Machine (SVM) is employed to determine the state of drowsiness. Besides, the voiced-to-unvoiced speech ratio has also been used. Source localization and synchronization of EEG signals have been employed for detection of various brain stages during various stages of fatigue and cross-validating the algorithms based in image and speech data. The synchronization has been represented in terms of a complex network and the parameters of the network have been used to trace the change in fatigue of sleep-deprived subjects. In addition, subjective feedback has also been obtained.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">A score level fusion method for eye movement biometrics</span>.
              <em>Pattern Recognition Letters</em>, 2016.
              <a href="https://doi.org/10.1016/j.patrec.2015.11.020" class="doi-link">doi:10.1016/j.patrec.2015.11.020</a> | <a href="https://www.semanticscholar.org/paper/caa18e294dff1ca9ecbf01e5b08b7fd164a42c03" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> No abstract available.</div>
                    </details><h2>2015</h2><details>
                      <summary>A. Dasgupta, <span class='bold'>Anjith George</span>, S. Happy, A. Routray.
              <span class="bold">A Vision Based System for Monitoring the Loss of Attention in Automotive Drivers</span>.
              <em>arXiv.org</em>, 2015.
              DOI not available | <a href="https://www.semanticscholar.org/paper/2f06615cc8f4ff3686b65a994480b08f6d69bbc0" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> : In this paper a real time vision based system is proposed to monitor driver fatigue. The whole system is built on the raspberry pi using the Raspbian operating system and OPENCV library for computer vision. The programming of the system is done by using C++ and PYTHON for GPIO programming of raspberry pi development board. The facial features are detected by Haar cascade classifier based on object detection algorithm. The eyes area are detected by using the function in the OpenCV library and tracking by using template matching method. . Vision-based driver fatigue detection method is a natural, non-intrusive and convenient technique to monitor driver’s vigilance. This dissertation attempts to study the driver’s drowsiness technique on Computer Vision (OpenCV) platform which is open source and developed by Intel.</div>
                    </details><details>
                      <summary>S. Happy, <span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">A real time facial expression classification system using Local Binary Patterns</span>.
              <em>International Conference on Intelligent Human Computer Interaction</em>, 2015.
              <a href="https://doi.org/10.1109/IHCI.2012.6481802" class="doi-link">doi:10.1109/IHCI.2012.6481802</a> | <a href="https://www.semanticscholar.org/paper/4b92e24678985feac4225386a9b9f4c76e09195b" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Facial expression analysis is one of the popular fields of research in human computer interaction (HCI). It has several applications in next generation user interfaces, human emotion analysis, behavior and cognitive modeling. In this paper, a facial expression classification algorithm is proposed which uses Haar classifier for face detection purpose, Local Binary Patterns(LBP) histogram of different block sizes of a face image as feature vectors and classifies various facial expressions using Principal Component Analysis (PCA). The algorithm is implemented in real time for expression classification since the computational complexity of the algorithm is small. A customizable approach is proposed for facial expression analysis, since the various expressions and intensity of expressions vary from person to person. The system uses grayscale frontal face images of a person to classify six basic emotions namely happiness, sadness, disgust, fear, surprise and anger.</div>
                    </details><details>
                      <summary>A. Dasgupta, Anshit Mandloi, <span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">An improved algorithm for eye corner detection</span>.
              <em>International Conference on Signal Processing and Communications</em>, 2015.
              <a href="https://doi.org/10.1109/SPCOM.2016.7746627" class="doi-link">doi:10.1109/SPCOM.2016.7746627</a> | <a href="https://www.semanticscholar.org/paper/9454399b92939a650f6cc967d129a71c9ffbb59c" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> In this paper, a modified algorithm for the detection of nasal and temporal eye corners is presented. The algorithm is a modification of the Santos and Proenka Method. In the first step, we detect the face and the eyes using classifiers based on Haar-like features. We then segment out the sclera, from the detected eye region. From the segmented sclera, we segment out an approximate eyelid contour. Eye corner candidates are obtained using Harris and Stephens corner detector. We introduce a post-pruning of the Eye corner candidates to locate the eye corners, finally. The algorithm has been tested on Yale, JAFFE databases as well as our created database.</div>
                    </details><details>
                      <summary>A. Dasgupta, <span class='bold'>Anjith George</span>, S. Happy, A. Routray.
              <span class="bold">An On-board Video Database of Human Drivers</span>.
              <em>arXiv.org</em>, 2015.
              DOI not available | <a href="https://www.semanticscholar.org/paper/b9fc34e913faddc1133b1d94c775455e5bd0aa15" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Detection of fatigue due to drowsiness or loss of attention in human drivers is an evolving area of research. Several algorithms have been implemented to detect the level of fatigue in human drivers by capturing videos of facial image sequences and extracting facial features such as eye closure rates, eye gaze, head nodding, blink frequency etc. However, availability of standard video database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under on-board conditions during the day as well as night. Passive Near Infra-red (NIR) illumination has been used for illuminating the face during night driving since prolonged exposure to active Infra-Red lighting may lead to many health issues. The database contains videos of 30 subjects under actual driving conditions. Variation is ensured as the database contains different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms for the video based detection of driver fatigue.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">Design and Implementation of Real-time Algorithms for Eye Tracking and PERCLOS Measurement for on board Estimation of Alertness of Drivers</span>.
              <em>arXiv.org</em>, 2015.
              DOI not available | <a href="https://www.semanticscholar.org/paper/bcc03b3522ad5bb4608359c56f747a02016e467f" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> The alertness level of drivers can be estimated with the use of computer vision based methods. The level of fatigue can be found from the value of PERCLOS. It is the ratio of closed eye frames to the total frames processed. The main objective of the thesis is the design and implementation of real-time algorithms for measurement of PERCLOS. In this work we have developed a real-time system which is able to process the video onboard and to alarm the driver in case the driver is in alert. For accurate estimation of PERCLOS the frame rate should be greater than 4 and accuracy should be greater than 90%. For eye detection we have used mainly two approaches Haar classifier based method and Principal Component Analysis (PCA) based method for day time. During night time active Near Infra Red (NIR) illumination is used. Local Binary Pattern (LBP) histogram based method is used for the detection of eyes at night time. The accuracy rate of the algorithms was found to be more than 90% at frame rates more than 5 fps which was suitable for the application.</div>
                    </details><details>
                      <summary><span class='bold'>Anjith George</span>, A. Dasgupta, A. Routray.
              <span class="bold">A Framework for Fast Face and Eye Detection</span>.
              <em>arXiv.org</em>, 2015.
              DOI not available | <a href="https://www.semanticscholar.org/paper/e5fa534e28e3511ea15a877f5cc8d86dba3ef2bb" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Face detection is an essential step in many computer vision applications like surveillance, tracking, medical analysis, facial expression analysis etc. Several approaches have been made in the direction of face detection. Among them, Haar-like features based method is a robust method. In spite of the robustness, Haar - like features work with some limitations. However, with some simple modifications in the algorithm, its performance can be made faster and more robust. The present work refers to the increase in speed of operation of the original algorithm by down sampling the frames and its analysis with different scale factors. It also discusses the detection of tilted faces using an affine transformation of the input image.</div>
                    </details><details>
                      <summary>A. Dasgupta, Bibek Kabi, <span class='bold'>Anjith George</span>, S. Happy, A. Routray.
              <span class="bold">A Drowsiness Detection Scheme Based on Fusion of Voice and Vision Cues</span>.
              <em>arXiv.org</em>, 2015.
              DOI not available | <a href="https://www.semanticscholar.org/paper/fb257360b71c0700ee9bedc93d11efab98c49184" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Drowsiness level detection of an individual is very important in many safety critical applications such as driving. There are several invasive and contact based methods such as use of blood biochemical, brain signals etc. which can estimate the level of drowsiness very accurately. However, these methods are very difficult to implement in practical scenarios, as they cause discomfort to the user. This paper presents a combined voice and vision based drowsiness detection system well suited to detect the drowsiness level of an automotive driver. The vision and voice based detection, being non-contact methods, has the advantage of their feasibility of implementation. The authenticity of these methods have been cross-validated using brain signals.</div>
                    </details><h2>2013</h2><details>
                      <summary>A. Dasgupta, <span class='bold'>Anjith George</span>, S. Happy, A. Routray, Tara Shanker.
              <span class="bold">An on-board vision based system for drowsiness detection in automotive drivers</span>.
              <em>International Journal of Advances in Engineering Sciences and Applied Mathematics</em>, 2013.
              <a href="https://doi.org/10.1007/s12572-013-0086-2" class="doi-link">doi:10.1007/s12572-013-0086-2</a> | <a href="https://www.semanticscholar.org/paper/575b4d109e6ab040347a606540740223d22f210d" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> No abstract available.</div>
                    </details><details>
                      <summary>A. Dasgupta, <span class='bold'>Anjith George</span>, S. Happy, A. Routray.
              <span class="bold">A Vision-Based System for Monitoring the Loss of Attention in Automotive Drivers</span>.
              <em>IEEE transactions on intelligent transportation systems (Print)</em>, 2013.
              <a href="https://doi.org/10.1109/TITS.2013.2271052" class="doi-link">doi:10.1109/TITS.2013.2271052</a> | <a href="https://www.semanticscholar.org/paper/a4ee76e6063195d9ff52ec0bdec681cb6640d7ef" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Onboard monitoring of the alertness level of an automotive driver has been challenging to research in transportation safety and management. In this paper, we propose a robust real-time embedded platform to monitor the loss of attention of the driver during day and night driving conditions. The percentage of eye closure has been used to indicate the alertness level. In this approach, the face is detected using Haar-like features and is tracked using a Kalman filter. The eyes are detected using principal component analysis during daytime and using the block local-binary-pattern features during nighttime. Finally, the eye state is classified as open or closed using support vector machines. In-plane and off-plane rotations of the driver's face have been compensated using affine transformation and perspective transformation, respectively. Compensation in illumination variation is carried out using bihistogram equalization. The algorithm has been cross-validated using brain signals and, finally, has been implemented on a single-board computer that has an Intel Atom processor with a 1.66-GHz clock, a random access memory of 1 GB, ×86 architecture, and a Windows-embedded XP operating system. The system is found to be robust under actual driving conditions.</div>
                    </details><h2>2012</h2><details>
                      <summary>S. Happy, A. Dasgupta, <span class='bold'>Anjith George</span>, A. Routray.
              <span class="bold">A video database of human faces under near Infra-Red illumination for human computer interaction applications</span>.
              <em>International Conference on Intelligent Human Computer Interaction</em>, 2012.
              <a href="https://doi.org/10.1109/IHCI.2012.6481868" class="doi-link">doi:10.1109/IHCI.2012.6481868</a> | <a href="https://www.semanticscholar.org/paper/33891ca0f8fab0eab503f4b4bcee009a1cf3b880" target="_blank">URL</a></summary>
              <div class="abstract-box"><span class="bold">Abstract:</span> Human Computer Interaction (HCI) is an evolving area of research for coherent communication between computers and human beings. Some of the important applications of HCI as reported in literature are face detection, face pose estimation, face tracking and eye gaze estimation. Development of algorithms for these applications is an active field of research. However, availability of standard database to validate such algorithms is insufficient. This paper discusses the creation of such a database created under Near Infra-Red (NIR) illumination. NIR illumination has gained its popularity for night mode applications since prolonged exposure to Infra-Red (IR) lighting may lead to many health issues. The database contains NIR videos of 60 subjects in different head orientations and with different facial expressions, facial occlusions and illumination variation. This new database can be a very valuable resource for development and evaluation of algorithms on face detection, eye detection, head tracking, eye gaze tracking etc. in NIR lighting.</div>
                    </details>
      </div>


    </div>

    


    <!-- /.container -->

    <!-- jQuery -->
    <script src="js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="js/bootstrap.min.js"></script>

</body>

</html>
